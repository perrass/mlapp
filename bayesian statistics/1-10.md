### Bayesian statistics syllabus

The goal of any sort of Bayesian inference process in general is **to derive what we call the posterior distribution**
$$
p(\theta | data) = {p(data|\theta)p(\theta)\over{p(data)}}
$$
where $p(data|\theta)$ is likelihood, the probability that we would have obtained that data given our choice of theta; $p(\theta)$ is the prior, the pre-experimetnal knowledge of the parameter values. A useful assumption is the prior is **uniform**. If the likelihood and prior is conjugate, this mean we can write closed-form posterior distribution

Three methods to gain insights to posterior distribution

* grid approximation
* metropolis hasting algorithms
* gibbs sampling

### Bayesian vs frequentist statistics

