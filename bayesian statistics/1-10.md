### Bayesian statistics syllabus

The goal of any sort of Bayesian inference process in general is **to derive what we call the posterior distribution**
$$
p(\theta | data) = {p(data|\theta)p(\theta)\over{p(data)}}
$$
where $p(data|\theta)$ is likelihood, the probability that we would have obtained that data given our choice of theta; $p(\theta)$ is the prior, the pre-experimetnal knowledge of the parameter values. A useful assumption is the prior is **uniform**. If the likelihood and prior is conjugate, this mean we can write closed-form posterior distribution

Three methods to gain insights to posterior distribution

* grid approximation
* metropolis hasting algorithms
* gibbs sampling

### Bayes' rule in inference

If the prior is uniform, this means **uninformative** prior

The denominator of bayes' rule can be omited, because it shared by all posterior. Hence
$$
p(\theta |data, Model) \sim p(data | \theta, Model) * p(\theta | Model)
$$
If the prior is uniform, **the maximum likelihood can be treated as the maximum posterior**

The denominator can be calculated as:
$$
p(data | M) = \sum_{\theta \in \Theta} p(\theta|M) \times p(data|\theta, M)
$$
The first param is prior and the later is likelihood

Continously:
$$
p(data|M) = \int_{\theta \in \Theta} p(\theta|M) \times p(data|\theta, M)d\theta
$$
Why likelihood is not a probability?

The likelihood of a parameter value, $\theta$, given outcomes x, is equal to the probability assumed for those observed outcomes given those parameter values, that is 
$$
l(\theta|x ) = P(x|\theta)
$$
but likelihood is not a probability
$$
\int_\infty^{-\infty}l(\theta|x)d\theta = [-\infty, \infty]
$$
but the integration of a probability is 1

### Exchangeability and iid

Exchangeability means the sequence of probability is not influent.

iid $\to$ exchangeablity, but exchangeability $\not \to$ iiid